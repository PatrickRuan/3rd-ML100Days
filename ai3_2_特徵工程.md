D22 ç‰¹å¾µå·¥ç¨‹ç°¡ä»‹_
ä»‹ç´¹æ©Ÿå™¨å­¸ç¿’å®Œæ•´æ­¥é©Ÿä¸­ï¼Œç‰¹å¾µå·¥ç¨‹çš„ä½ç½®ä»¥åŠæµç¨‹æ¶æ§‹

D23 æ•¸å€¼å‹ç‰¹å¾µ - å»é™¤åæ…‹_
æ•¸å€¼å‹ç‰¹å¾µè‹¥åˆ†å¸ƒæ˜é¡¯åä¸€é‚Šï¼Œå‰‡éœ€å»é™¤åæ…‹ä»¥æ¶ˆé™¤é æ¸¬çš„åå·®

D24 é¡åˆ¥å‹ç‰¹å¾µ - åŸºç¤è™•ç†
ä»‹ç´¹é¡åˆ¥å‹ç‰¹å¾µæœ€åŸºç¤çš„ä½œæ³• : æ¨™ç±¤ç·¨ç¢¼èˆ‡ç¨ç†±ç·¨ç¢¼

D25 é¡åˆ¥å‹ç‰¹å¾µ - å‡å€¼ç·¨ç¢¼
é¡åˆ¥å‹ç‰¹å¾µæœ€é‡è¦çš„ç·¨ç¢¼ : å‡å€¼ç·¨ç¢¼ï¼Œå°‡æ¨™ç±¤ä»¥ç›®æ¨™å‡å€¼å–ä»£

D26é¡åˆ¥å‹ç‰¹å¾µ - å…¶ä»–é€²éšè™•ç†
é¡åˆ¥å‹ç‰¹å¾µçš„å…¶ä»–å¸¸è¦‹ç·¨ç¢¼ : è¨ˆæ•¸ç·¨ç¢¼å°æ‡‰å‡ºç¾é »ç‡ç›¸é—œçš„ç‰¹å¾µï¼Œé›œæ¹Šç·¨ç¢¼å°æ‡‰çœ¾å¤šé¡åˆ¥è€Œç„¡æ³•æ’åºçš„ç‰¹å¾µ

D27 æ™‚é–“å‹ç‰¹å¾µ
æ™‚é–“å‹ç‰¹å¾µå¯æŠ½å–å‡ºå¤šå€‹å­ç‰¹å¾µï¼Œæˆ–å‘¨æœŸåŒ–ï¼Œæˆ–å–å‡ºé€£çºŒæ™‚æ®µå…§çš„æ¬¡æ•¸

D28 ç‰¹å¾µçµ„åˆ - æ•¸å€¼èˆ‡æ•¸å€¼çµ„åˆ
ç‰¹å¾µçµ„åˆçš„åŸºç¤ : ä»¥å››å‰‡é‹ç®—çš„å„ç¨®æ–¹å¼ï¼Œçµ„åˆæˆæ›´å…·é æ¸¬åŠ›çš„ç‰¹å¾µ

D29 ç‰¹å¾µçµ„åˆ - é¡åˆ¥èˆ‡æ•¸å€¼çµ„åˆ
é¡åˆ¥å‹å°æ•¸å€¼å‹ç‰¹å¾µå¯ä»¥åšç¾¤èšç·¨ç¢¼ï¼Œèˆ‡ç›®æ¨™å‡å€¼ç·¨ç¢¼é¡ä¼¼ï¼Œä½†ç”¨é€”ä¸åŒğŸ

D30 ç‰¹å¾µé¸æ“‡
ä»‹ç´¹å¸¸è¦‹çš„å¹¾ç¨®ç‰¹å¾µç¯©é¸æ–¹å¼

D31 ç‰¹å¾µè©•ä¼°
ä»‹ç´¹ä¸¦æ¯”è¼ƒå…©ç¨®é‡è¦çš„ç‰¹å¾µè©•ä¼°æ–¹å¼ï¼Œå”åŠ©æª¢æ¸¬ç‰¹å¾µçš„é‡è¦æ€§

D32 åˆ†é¡å‹ç‰¹å¾µå„ªåŒ– - è‘‰ç·¨ç¢¼
è‘‰ç·¨ç¢¼ : é©ç”¨æ–¼åˆ†é¡å•é¡Œçš„æ¨¹ç‹€é ä¼°æ¨¡å‹æ”¹è‰¯














å‡å€¼ç·¨ç¢¼

    data = pd.concat([df[:train_num], train_Y], axis=1)
    for c in df.columns:
        mean_df = data.groupby([c])['Survived'].mean().reset_index()
        mean_df.columns = [c, f'{c}_mean']
        data = pd.merge(data, mean_df, on=c, how='left')
        data = data.drop([c] , axis=1)
    data = data.drop(['Survived'] , axis=1)
    estimator = LogisticRegression()
    start = time.time()
    print(f'shape : {X.shape}')
    print(f'score : {cross_val_score(estimator, data, train_Y, cv=5).mean()}')
    print(f'time : {time.time() - start} sec')
    
    é€™ä¸€æ®µ code å› ç‚º ticket_mean, name_mean é …ç›®å¤ªå¤šï¼Œæ¯ä¸€å€‹éƒ½è¢«æ¨™ç±¤äº† target, è‡ªç„¶æ˜¯ overfitting.
    ç•™æ„çš„æ˜¯ï¼Œæˆ‘å€‘çš„ cross_val_score ç”¨åœ¨ training dataï¼Œæ‰€ä»¥æ•ˆæœéƒ½æ˜¯å¥½çš„ã€‚(?)
    é‚„æœ‰, meanvalueencoding ç®—ä¸ç®—æ´©æ¼ target???


D26  # è§€å¯Ÿæ¬„ä½ç›¸ç•°å€¼æ•¸é‡
    
    df.select_dtypes(include=["object"]).apply(pd.Series.nunique)
    
    count_df = df.groupby(['Cabin'])['Name'].agg({'Cabin_Count':'size'}).reset_index()
    df = pd.merge(df, count_df, on=['Cabin'], how='left')
    count_df.sort_values(by=['Cabin_Count'], ascending=False).head(10)
    df_temp['Cabin_Hash'] = df['Cabin'].map(lambda x:hash(x) % 10)
    
    
    
D27 #

    df['pickup_datetime'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S UTC'))
    df['pickup_year'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%Y')).astype('int64')
    df['pickup_month'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%m')).astype('int64')
    df['pickup_day'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%d')).astype('int64')
    df['pickup_hour'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%H')).astype('int64')
    df['pickup_minute'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%M')).astype('int64')
    df['pickup_second'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%S')).astype('int64')
